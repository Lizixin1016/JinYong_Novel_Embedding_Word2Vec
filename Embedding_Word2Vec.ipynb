{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入必要的库 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置参数，包含数据路径、模型参数等 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    data_folder = \"D:\\\\大三下\\\\Word2Vec\\\\Novel_Database\"  # 小说文件夹\n",
    "    martial_arts_file = \"D:\\\\大三下\\\\Word2Vec\\\\stop\\\\金庸小说全武功.txt\"  # 武功\n",
    "    factions_file = \"D:\\\\大三下\\\\Word2Vec\\\\stop\\\\金庸小说全门派.txt\"  # 门派\n",
    "    characters_file = \"D:\\\\大三下\\\\Word2Vec\\\\stop\\\\金庸小说全人物.txt\"  # 人物\n",
    "    stopwords_file = \"D:\\\\大三下\\\\Word2Vec\\\\stop\\\\stop_words.txt\"  # 停用词\n",
    "\n",
    "\n",
    "    embedding_dim = 200  # 词向量维度\n",
    "    window_size = 5  # 上下文窗口\n",
    "    batch_size = 512  # CPU 上减小 batch_size\n",
    "    num_epochs = 5  # 训练轮次\n",
    "    min_count = 5  # 最低词频\n",
    "    neg_samples = 5  # 负采样数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测文件编码 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting encoding for {file_path}: {e}\")\n",
    "        return 'utf-8'  # 默认使用 utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载停用词 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    #使用gbk编码\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='gbk') as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    except UnicodeDecodeError:\n",
    "        # 若 gbk 失败，尝试使用 chardet 检测的编码\n",
    "        encoding = detect_encoding(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return set(line.strip() for line in f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading stopwords from {file_path}: {e}\")\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载特殊词汇（人物、武功、门派） ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_special_words(file_path):\n",
    "    # 使用gbk编码\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='gbk') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except UnicodeDecodeError:\n",
    "        # 若 utf-8 失败，尝试使用 chardet 检测的编码\n",
    "        encoding = detect_encoding(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return set(line.strip() for line in f if line.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading special words from {file_path}: {e}\")\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取小说并预处理 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_novels(folder_path, characters, martial_arts, factions, stopwords):\n",
    "    all_words = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            encoding = detect_encoding(file_path)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    text = f.read()\n",
    "                    # 合并特殊词汇（人物、武功、门派）\n",
    "                    special_words = characters | martial_arts | factions\n",
    "                    pattern = '|'.join(map(re.escape, sorted(special_words, key=len, reverse=True)))\n",
    "                    text = re.sub(f'({pattern})', r' \\1 ', text)\n",
    "                    # 分词\n",
    "                    words = re.findall(r'[\\w\\u4e00-\\u9fff]+', text)\n",
    "                    words = [word for word in words if word not in stopwords]\n",
    "                    all_words.extend(words)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建词汇表（确保索引从0连续） ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_words, min_count):\n",
    "    word_counts = Counter(all_words)\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if count >= min_count}\n",
    "    vocab = {word: idx for idx, word in enumerate(filtered_words)}\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    print(f\"Vocab size: {len(vocab)} (min_count={min_count})\")\n",
    "    return vocab, idx_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样 Word2Vec 模型 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 初始化权重\n",
    "        self.input_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "        self.output_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "\n",
    "    def forward(self, target, context, neg_samples):\n",
    "        # 检查索引范围\n",
    "        if (target >= self.vocab_size).any() or (context >= self.vocab_size).any() or (\n",
    "                neg_samples >= self.vocab_size).any():\n",
    "            raise ValueError(f\"Index out of range (vocab_size={self.vocab_size})\")\n",
    "\n",
    "        # 正样本\n",
    "        emb_target = self.input_embeddings(target)  # (batch_size, emb_dim)\n",
    "        emb_context = self.output_embeddings(context)  # (batch_size, emb_dim)\n",
    "        pos_score = torch.sum(emb_target * emb_context, dim=1)  # (batch_size)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        # 负样本\n",
    "        neg_emb = self.output_embeddings(neg_samples)  # (batch_size, neg_samples, emb_dim)\n",
    "        neg_score = torch.bmm(neg_emb, emb_target.unsqueeze(2)).squeeze()  # (batch_size, neg_samples)\n",
    "        neg_loss = F.logsigmoid(-neg_score).mean(dim=1)\n",
    "\n",
    "        return -(pos_loss + neg_loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备训练数据（严格检查索引范围） ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(all_words, vocab, window_size):\n",
    "    data = []\n",
    "    vocab_size = len(vocab)\n",
    "    for i, target_word in enumerate(tqdm(all_words, desc=\"Preparing data\")):\n",
    "        if target_word not in vocab:\n",
    "            continue\n",
    "        target_idx = vocab[target_word]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(all_words), i + window_size + 1)\n",
    "        context_words = all_words[start:i] + all_words[i + 1:end]\n",
    "\n",
    "        for context_word in context_words:\n",
    "            if context_word in vocab:\n",
    "                context_idx = vocab[context_word]\n",
    "                if context_idx < vocab_size:  # 双重检查\n",
    "                    data.append((target_idx, context_idx))\n",
    "\n",
    "    print(f\"Generated {len(data)} training pairs\")\n",
    "    return np.array(data, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成负样本 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_samples(context, vocab_size, neg_samples):\n",
    "    neg = []\n",
    "    for c in context:\n",
    "        while True:\n",
    "            sample = np.random.randint(0, vocab_size, neg_samples)\n",
    "            if not np.any(sample == c):\n",
    "                neg.append(sample)\n",
    "                break\n",
    "    return np.array(neg, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, train_data, vocab_size, batch_size, num_epochs, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            batch = train_data[i:i + batch_size]\n",
    "            targets = torch.tensor(batch[:, 0], dtype=torch.long).to(device)\n",
    "            contexts = torch.tensor(batch[:, 1], dtype=torch.long).to(device)\n",
    "            neg_samples = torch.tensor(generate_neg_samples(contexts.cpu().numpy(), vocab_size, Config.neg_samples)).to(\n",
    "                device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(targets, contexts, neg_samples)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Avg Loss: {total_loss / len(train_data):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化（自动调整标签密度） ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, words, title, dim=2):\n",
    "    # 设置支持中文的字体\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体字体\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "    pca = PCA(n_components=dim)\n",
    "    # 确保 embeddings 是二维数组\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)\n",
    "    reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    if dim == 2:\n",
    "        # 绘制散点\n",
    "        scatter = plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.6, s=50)\n",
    "\n",
    "        # 智能标注（避免重叠）\n",
    "        texts = []\n",
    "        for i, (x, y) in enumerate(zip(reduced[:, 0], reduced[:, 1])):\n",
    "            texts.append(plt.text(x, y, words[i], fontsize=9,\n",
    "                                  bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')))\n",
    "        adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red', lw=0.5), lim=50,  # 减少迭代次数，这里是，体量太大，内存不够了\n",
    "                    force_text=0.1,  # 减少文本之间的排斥力\n",
    "                    force_points=0.1)\n",
    "\n",
    "    elif dim == 3:\n",
    "        ax = plt.axes(projection='3d')\n",
    "        scatter = ax.scatter3D(reduced[:, 0], reduced[:, 1], reduced[:, 2], alpha=0.6, s=50)\n",
    "\n",
    "        # 3D标注（仅显示部分避免混乱）\n",
    "        for i in range(0, len(words),10):  \n",
    "            ax.text(reduced[i, 0], reduced[i, 1], reduced[i, 2], words[i],\n",
    "                    fontsize=9, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"word2vec_{dim}d.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载停用词和特殊词汇\n",
    "    stopwords = load_stopwords(Config.stopwords_file)\n",
    "    characters = load_special_words(Config.characters_file)\n",
    "    martial_arts = load_special_words(Config.martial_arts_file)\n",
    "    factions = load_special_words(Config.factions_file)\n",
    "\n",
    "    # 读取并预处理小说\n",
    "    all_words = load_and_preprocess_novels(Config.data_folder, characters, martial_arts, factions, stopwords)\n",
    "\n",
    "    # 构建词汇表\n",
    "    vocab, idx_to_word = build_vocab(all_words, Config.min_count)\n",
    "\n",
    "    # 准备训练数据\n",
    "    train_data = prepare_training_data(all_words, vocab, Config.window_size)\n",
    "\n",
    "    # 初始化模型\n",
    "    vocab_size = len(vocab)\n",
    "    model = SkipGramNeg(vocab_size, Config.embedding_dim)\n",
    "\n",
    "    # 训练模型\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_word2vec(model, train_data, vocab_size, Config.batch_size, Config.num_epochs, device)\n",
    "\n",
    "    # 获取所有人名的嵌入\n",
    "    all_character_embeddings = []\n",
    "    all_character_names = []\n",
    "    selected_characters = list(characters)[:1000]  # 只选择前1000个人名，由于内存不够，所以只取了前一千\n",
    "    for character in selected_characters:\n",
    "        if character in vocab:\n",
    "            idx = vocab[character]\n",
    "            embedding = model.input_embeddings.weight[idx].detach().cpu().numpy()\n",
    "            all_character_embeddings.append(embedding)\n",
    "            all_character_names.append(character)\n",
    "    all_character_embeddings = np.array(all_character_embeddings)\n",
    "\n",
    "    # 可视化人名嵌入（二维）\n",
    "    visualize_embeddings(all_character_embeddings, all_character_names, \"Character Embeddings (2D)\", dim=2)\n",
    "\n",
    "    # 可视化人名嵌入（三维）\n",
    "    visualize_embeddings(all_character_embeddings, all_character_names, \"Character Embeddings (3D)\", dim=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
